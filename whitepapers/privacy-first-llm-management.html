<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Privacy-First Federated LLM Management Architecture - Cleansheet LLC White Paper</title>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Barlow:wght@300&family=Questrial&display=swap" rel="stylesheet">

    <!-- Mermaid.js for Diagram Rendering -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>

    <style>
        /* Cleansheet Design System - Corporate Professional */
        :root {
            --color-primary-blue: #0066CC;
            --color-accent-blue: #004C99;
            --color-dark: #1a1a1a;
            --color-neutral-text: #333333;
            --color-neutral-text-light: #666666;
            --color-neutral-text-muted: #999999;
            --color-neutral-background: #f5f5f7;
            --color-neutral-background-secondary: #f8f8f8;
            --color-neutral-border: #e5e5e7;
            --color-neutral-white: #ffffff;
            --color-semantic-success: #28a745;
            --color-semantic-warning: #ffc107;
            --color-semantic-error: #dc3545;
            --color-semantic-info: #007acc;
            --font-family-ui: 'Questrial', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            --font-family-body: 'Barlow', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            --font-size-h1: clamp(28px, 4vw, 32px);
            --font-size-h2: clamp(24px, 3.5vw, 28px);
            --font-size-h3: clamp(18px, 3vw, 24px);
            --font-size-h4: clamp(16px, 2.8vw, 20px);
            --font-size-body: clamp(14px, 2.5vw, 16px);
            --font-size-small: clamp(12px, 2.2vw, 14px);
            --spacing-xs: 4px;
            --spacing-sm: 8px;
            --spacing-md: 12px;
            --spacing-lg: 16px;
            --spacing-xl: 20px;
            --spacing-xxl: 24px;
            --spacing-xxxl: 32px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-family-body);
            font-weight: 300;
            font-size: var(--font-size-body);
            line-height: 1.6;
            color: var(--color-neutral-text);
            background: var(--color-neutral-background);
        }

        .header {
            background: var(--color-dark);
            color: var(--color-neutral-white);
            padding: var(--spacing-xxxl) 0;
            margin-bottom: var(--spacing-xxxl);
        }

        .header-content {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 var(--spacing-xxl);
        }

        h1 {
            font-family: var(--font-family-ui);
            font-size: var(--font-size-h1);
            font-weight: 600;
            margin-bottom: var(--spacing-lg);
            text-align: center;
            border-bottom: 4px solid var(--color-primary-blue);
            padding-bottom: var(--spacing-lg);
        }

        .publication-info {
            font-family: var(--font-family-body);
            font-size: var(--font-size-small);
            line-height: 1.8;
            text-align: center;
            margin-top: var(--spacing-lg);
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 var(--spacing-xxl) var(--spacing-xxxl);
        }

        .section {
            margin: var(--spacing-xxxl) 0;
            padding: var(--spacing-xxl);
            background: var(--color-neutral-white);
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .abstract {
            background: var(--color-neutral-background-secondary);
            border-left: 4px solid var(--color-primary-blue);
            font-style: italic;
        }

        h2 {
            font-family: var(--font-family-ui);
            font-size: var(--font-size-h2);
            font-weight: 600;
            color: var(--color-primary-blue);
            margin: var(--spacing-xxl) 0 var(--spacing-lg);
            border-left: 4px solid var(--color-primary-blue);
            padding-left: var(--spacing-lg);
        }

        h3 {
            font-family: var(--font-family-ui);
            font-size: var(--font-size-h3);
            font-weight: 600;
            color: var(--color-accent-blue);
            margin: var(--spacing-xl) 0 var(--spacing-md);
        }

        h4 {
            font-family: var(--font-family-ui);
            font-size: var(--font-size-h4);
            font-weight: 600;
            color: var(--color-dark);
            margin: var(--spacing-lg) 0 var(--spacing-sm);
        }

        p {
            margin: var(--spacing-md) 0;
            line-height: 1.6;
        }

        ul, ol {
            margin: var(--spacing-md) 0 var(--spacing-md) var(--spacing-xxl);
        }

        li {
            margin: var(--spacing-sm) 0;
            line-height: 1.6;
        }

        .key-features {
            background: linear-gradient(135deg, var(--color-primary-blue), var(--color-accent-blue));
            color: var(--color-neutral-white);
            padding: var(--spacing-xxl);
            border-radius: 8px;
            margin: var(--spacing-xxl) 0;
        }

        .key-features h3 {
            color: var(--color-neutral-white);
        }

        .key-features ul {
            list-style-type: none;
            margin-left: 0;
        }

        .key-features li:before {
            content: "✓ ";
            font-weight: bold;
            margin-right: var(--spacing-sm);
        }

        .pseudocode {
            background: var(--color-neutral-background);
            border: 1px solid var(--color-neutral-border);
            border-radius: 4px;
            padding: var(--spacing-lg);
            margin: var(--spacing-lg) 0;
            font-family: 'Courier New', Consolas, monospace;
            font-size: var(--font-size-small);
            line-height: 1.4;
            color: var(--color-dark);
            overflow-x: auto;
            white-space: pre-wrap;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: var(--spacing-lg) 0;
        }

        th {
            background: var(--color-neutral-background);
            font-family: var(--font-family-ui);
            font-weight: 600;
            color: var(--color-dark);
            text-align: left;
            padding: var(--spacing-md);
            border-bottom: 2px solid var(--color-primary-blue);
        }

        td {
            text-align: left;
            padding: var(--spacing-md);
            border-bottom: 1px solid var(--color-neutral-border);
        }

        .figure {
            margin: var(--spacing-xxl) 0;
            padding: var(--spacing-lg);
            border: 1px solid var(--color-neutral-border);
            border-radius: 8px;
            background: var(--color-neutral-background-secondary);
        }

        .figure-title {
            font-family: var(--font-family-ui);
            font-weight: 600;
            color: var(--color-dark);
            margin-bottom: var(--spacing-md);
            font-size: var(--font-size-h4);
        }

        .figure-description {
            font-family: var(--font-family-body);
            font-size: var(--font-size-small);
            color: var(--color-neutral-text-light);
            margin-top: var(--spacing-md);
            font-style: italic;
        }

        a {
            color: var(--color-primary-blue);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .container {
                padding: 0 var(--spacing-lg) var(--spacing-xxl);
            }
            .section {
                padding: var(--spacing-lg);
            }
            .header-content {
                padding: 0 var(--spacing-lg);
            }
            .pseudocode {
                font-size: 12px;
                padding: var(--spacing-md);
            }
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="header-content">
            <h1>Privacy-First Federated LLM Management Architecture for Bring-Your-Own-Key Environments</h1>
            <p class="publication-info">
                <strong>Publication Date:</strong> November 16, 2025<br>
                <strong>Version:</strong> 1.0<br>
                <strong>Author:</strong> Cleansheet LLC<br>
                <strong>Contact:</strong> cleansheet.info
            </p>
        </div>
    </header>

    <div class="container">
        <!-- ABSTRACT -->
        <section class="section abstract">
            <h2>Abstract</h2>
            <p>As organizations integrate Large Language Model (LLM) capabilities into their platforms, they face escalating regulatory, privacy, and security exposure from managing API keys, storing conversation data, and acting as intermediaries between users and AI providers. Traditional managed LLM architectures require platforms to handle sensitive credentials, custody conversation content, and assume liability for data processing—creating substantial compliance burdens under GDPR, HIPAA, CCPA, and SOC 2 frameworks. This white paper presents a <strong>Privacy-First Federated LLM Management Architecture</strong> based on Bring-Your-Own-Key (BYOK) principles that dramatically minimizes platform exposure while providing users complete control and flexibility.</p>

            <p>The architecture implements a zero-server communication pattern where user browsers connect directly to LLM providers (OpenAI, Anthropic, Azure OpenAI, Google Gemini) without platform intermediation. API keys are encrypted client-side using AES-GCM 256-bit encryption with PBKDF2 key derivation (100,000 iterations) and stored exclusively in browser localStorage—never transmitted to platform servers. Conversations exist only in browser memory during active sessions with no persistent storage, eliminating the primary data breach vector in AI-integrated systems. A provider abstraction layer enables seamless multi-provider support without vendor lock-in, while informational usage tracking (stored locally) provides transparency without quota enforcement.</p>

            <p>This architecture achieves substantial compliance benefits: GDPR does not apply as the platform performs no data processing; HIPAA Business Associate Agreements (BAAs) are unnecessary as the platform never custody Protected Health Information (PHI); CCPA obligations are minimal with no data sale or sharing; and SOC 2 audit scope is dramatically reduced. Organizations implementing this pattern eliminate regulatory exposure from LLM integration while giving users choice, control, and cost management. The system supports both consumer and enterprise deployment models including individual BYOK, enterprise proxy mode with shared corporate keys, and hybrid tiers combining BYOK for sensitive use with managed access for casual interactions.</p>

            <p>This architectural pattern establishes prior art for federated LLM management systems that prioritize user sovereignty and platform liability minimization while maintaining full functional capabilities. It provides a reference implementation for any organization requiring AI integration without accepting the regulatory burden of data custody and processing.</p>

            <p><strong>Keywords:</strong> BYOK, privacy-first architecture, federated LLM, client-side encryption, AES-GCM, zero-server pattern, GDPR compliance, HIPAA compliance, regulatory minimization, API key management, Web Crypto API, multi-provider abstraction, data sovereignty, minimal platform liability</p>
        </section>

        <!-- 1. TECHNICAL FIELD -->
        <section class="section">
            <h2>1. Technical Field</h2>

            <h3>1.1 Background</h3>
            <p>Large Language Models have become essential infrastructure for modern software applications, enabling intelligent features including content generation, analysis, summarization, translation, and conversational interfaces. Organizations across industries—from healthcare and legal services to education and enterprise software—are integrating LLM capabilities to enhance user experiences and operational efficiency.</p>

            <p>However, LLM integration introduces significant technical and regulatory challenges. Platforms must decide how to provision LLM access: through managed services where the platform provides API keys and proxies requests, or through federated models where users bring their own credentials. Managed approaches create substantial platform liabilities:</p>

            <ul>
                <li><strong>API Key Custody:</strong> Platforms storing user API keys assume responsibility for credential security, with breach exposure including financial loss and reputation damage</li>
                <li><strong>Conversation Data Custody:</strong> Platforms that log, store, or analyze conversations become data controllers under GDPR, data processors under HIPAA, and businesses under CCPA</li>
                <li><strong>Cost Burden:</strong> Platform-provided LLM access requires absorbing API costs, implementing billing systems, and forecasting usage—complex financial management</li>
                <li><strong>Vendor Lock-In:</strong> Deep integration with specific LLM providers creates migration challenges when better models, pricing, or compliance features emerge</li>
                <li><strong>Compliance Complexity:</strong> Acting as data processor triggers extensive regulatory obligations including Data Processing Agreements (DPAs), audit requirements, breach notification procedures, and ongoing monitoring</li>
            </ul>

            <p>The Web Crypto API (W3C standard implemented in all modern browsers) provides cryptographic primitives enabling secure client-side operations without server involvement. Modern browser localStorage offers sufficient capacity (5-10MB) for application state management. Fetch API enables direct HTTPS requests from browsers to any external service. These capabilities create technical foundation for federated architectures where platforms facilitate functionality without becoming data custodians.</p>

            <h3>1.2 Problem Statement</h3>
            <p>Current LLM integration approaches exhibit critical limitations that expose platforms to unnecessary risk:</p>

            <ul>
                <li><strong>Centralized API Key Storage:</strong> Platforms that store user API keys in databases (even encrypted) become high-value attack targets. A single database breach exposes all user credentials. Encryption key management becomes critical infrastructure requiring Hardware Security Modules (HSMs) or cloud key management services, adding cost and complexity.</li>

                <li><strong>Server-Side Conversation Logging:</strong> Many platforms log LLM conversations for debugging, analytics, or feature development. This creates massive data custody obligations. Under GDPR, the platform becomes a data controller requiring DPAs with users. Under HIPAA, conversation logs containing PHI require BAAs with users and encryption at rest. Under CCPA, conversation data may constitute "personal information" triggering disclosure and deletion rights.</li>

                <li><strong>Proxy Architecture Latency:</strong> Traditional architectures route LLM requests through platform servers for authentication, rate limiting, or logging. This adds network latency (typically 50-200ms per hop) degrading user experience, especially for streaming responses where every millisecond affects perceived responsiveness.</li>

                <li><strong>Cost Unpredictability:</strong> Platforms providing managed LLM access must forecast usage and set pricing. Underestimating creates financial loss; overestimating reduces competitiveness. Heavy users can generate unexpected costs. Implementing quota systems requires complex billing infrastructure and creates friction with users.</li>

                <li><strong>Vendor Lock-In Risk:</strong> Deep integration with specific LLM providers (OpenAI, Anthropic, etc.) through server-side SDKs creates migration challenges. When providers change pricing, deprecate models, or modify terms of service, platforms face expensive refactoring. Users cannot leverage corporate agreements with alternative providers.</li>

                <li><strong>Compliance Audit Scope:</strong> Platforms acting as data processors must undergo extensive compliance audits. SOC 2 Type II audits examining LLM integration can add $50,000-150,000 annually in audit costs. HIPAA compliance requires technical safeguards, administrative procedures, and physical security measures. Each jurisdiction with data protection laws (GDPR in EU, LGPD in Brazil, PIPEDA in Canada) triggers separate compliance requirements.</li>

                <li><strong>Data Residency Constraints:</strong> Enterprise customers often require data remain within specific geographic regions (EU data in EU datacenters, US government data in US). Managed LLM architectures require platform infrastructure in every target region. BYOK approaches allow users to choose providers meeting their residency requirements without platform infrastructure changes.</li>
            </ul>

            <p>These problems create a fundamental tension: AI capabilities are essential for competitive products, but traditional integration approaches impose crushing regulatory burdens especially on startups and small platforms lacking dedicated compliance teams.</p>

            <h3>1.3 Prior Art</h3>
            <p>Several approaches to LLM integration exist in prior art, each with significant limitations:</p>

            <p><strong>Fully Managed LLM Services:</strong> Platforms like Intercom, Zendesk AI, or Notion AI provide LLM capabilities fully managed by the platform. Users interact with AI features without providing credentials. The platform handles all API communication, cost, and data management. However, these systems expose the platform to maximum liability—full data custody, all regulatory compliance obligations, complete cost burden, and no user choice of providers. These platforms must implement comprehensive security controls, undergo extensive audits, and maintain compliance across all jurisdictions where they operate.</p>

            <p><strong>API Key Configuration in Settings:</strong> Some platforms allow users to enter LLM API keys in account settings, which are then stored in platform databases (typically encrypted). The platform retrieves these keys to make API calls on the user's behalf. While this shifts cost to users, the platform still custody credentials (breach risk), acts as proxy adding latency, and maintains server logs creating data custody obligations. Encryption key management remains platform responsibility. This approach reduces cost burden but maintains most regulatory exposure.</p>

            <p><strong>OAuth Delegation:</strong> Systems like GitHub Copilot use OAuth flows where users authorize the platform to access LLM providers on their behalf. This eliminates explicit API key storage but requires OAuth integration with each provider (many LLM providers don't offer OAuth), platform still proxies requests creating latency and logging concerns, and tokens must be stored and refreshed (similar custody concerns to API keys).</p>

            <p><strong>Client-Side Direct Integration:</strong> Some applications embed LLM provider SDKs (JavaScript libraries) that make direct API calls from the browser. However, existing implementations fail to address key management—they either hardcode API keys in JavaScript (severe security vulnerability) or require users to paste keys into form fields each session (poor UX). They lack multi-provider abstraction requiring separate implementations for each LLM service. They provide no usage tracking or cost visibility. They offer no guidance on conversation data management.</p>

            <p><strong>Edge Function Proxies:</strong> Architectures using Cloudflare Workers, AWS Lambda@Edge, or similar edge computing to proxy LLM requests closer to users reduce latency compared to centralized servers. However, they maintain fundamental problems: platform still custody API keys (even if stored in edge key-value stores), logs conversation data for debugging, acts as data processor under regulations, and incurs infrastructure costs that scale with usage.</p>

            <p><strong>Self-Hosted LLM Solutions:</strong> Some organizations deploy models like Llama, Mistral, or GPT-J on their own infrastructure. This maximizes data control but requires significant technical expertise (model deployment, GPU infrastructure, fine-tuning), substantial cost (GPU servers, inference optimization), and model quality trade-offs (self-hosted models typically underperform commercial APIs). This approach suits organizations with deep ML expertise and specific data isolation requirements but is impractical for most platforms.</p>

            <p>None of these prior art systems achieve: (1) zero platform custody of API keys through client-side encryption, (2) zero platform custody of conversation data through session-only memory, (3) zero-latency proxy through direct browser-provider communication, (4) zero cost burden through pure BYOK without managed tiers, (5) multi-provider abstraction enabling user choice, and (6) comprehensive compliance liability minimization. This white paper addresses all these requirements through a novel federated architecture.</p>
        </section>

        <!-- 2. SUMMARY OF INVENTION -->
        <section class="section">
            <h2>2. Summary of the Invention</h2>

            <h3>2.1 Overview</h3>
            <p>The Privacy-First Federated LLM Management Architecture fundamentally reimagines the relationship between platforms, users, and AI providers. Rather than positioning the platform as intermediary and data custodian, this architecture positions the platform as <strong>facilitator</strong> providing user interface and coordination while <strong>users maintain direct relationships with AI providers</strong>. The platform never sees API keys, never stores conversations, never proxies requests, and therefore avoids nearly all regulatory obligations associated with LLM integration.</p>

            <p>At the heart of this architecture is <strong>client-side AES-GCM encryption</strong> for API key storage. When users configure LLM access, their API keys are encrypted in the browser using Web Crypto API with 256-bit keys derived from user-specific identifiers through PBKDF2 (100,000 iterations). Encrypted keys are stored in browser localStorage. Decryption occurs only in browser memory when needed for API calls. Keys never traverse network connections to platform servers in any form—encrypted or otherwise.</p>

            <p>The <strong>zero-server communication pattern</strong> eliminates platform intermediation entirely. When users interact with AI features, their browsers make direct HTTPS requests to LLM provider APIs (api.openai.com, api.anthropic.com, etc.) using Fetch API. Responses stream directly back to the browser. The platform serves static HTML/CSS/JavaScript that orchestrates this interaction, but no platform server participates in the data flow. This eliminates proxy latency, removes server logging opportunities, and ensures platforms never custody conversation content.</p>

            <p>A <strong>provider abstraction layer</strong> implements the Strategy pattern enabling users to choose from multiple LLM providers without application code changes. Abstract base classes define common interfaces (chat, streamChat, validateApiKey, getModels). Concrete implementations for OpenAI, Anthropic, Azure OpenAI, and Google Gemini encapsulate provider-specific API details. Users switch providers through UI configuration. New providers are added by implementing the abstract interface without modifying application code.</p>

            <p><strong>Session-only conversation memory</strong> maintains conversation context in browser JavaScript variables during active user sessions. When users close browsers or end sessions, conversation arrays are garbage collected with no persistence. This eliminates the primary data breach vector in AI systems (leaked conversation logs) while maintaining functional conversational capabilities during active use. Users retain full control over conversation lifecycle with zero platform liability.</p>

            <div class="key-features">
                <h3>2.2 Key Features</h3>
                <ul>
                    <li><strong>Client-Side AES-GCM Encryption:</strong> API keys encrypted in browser using Web Crypto API with 256-bit keys derived via PBKDF2 (100,000 iterations) from user-specific identifiers. Encrypted keys stored in localStorage. Decryption occurs only in memory for API calls. Platform servers never see keys in any form. Eliminates credential breach risk and key management infrastructure requirements.</li>

                    <li><strong>Zero-Server Communication Pattern:</strong> User browsers connect directly to LLM provider APIs (api.openai.com, api.anthropic.com) via Fetch API. No platform proxy, no middleware, no server logging. Platform serves static assets orchestrating client-side logic. Eliminates data custody, reduces latency by 50-200ms, removes server infrastructure requirements for LLM integration.</li>

                    <li><strong>Session-Only Conversation Memory:</strong> Conversations maintained in browser JavaScript arrays during active sessions. No localStorage persistence, no server transmission, automatic garbage collection on browser close. Eliminates primary data breach vector (conversation logs) while maintaining functional conversational context during use. User controls lifecycle completely.</li>

                    <li><strong>Multi-Provider Abstraction Layer:</strong> Strategy pattern implementation with abstract LLMProvider base class and concrete implementations for OpenAI, Anthropic, Azure OpenAI, Google Gemini. Unified interface (chat, streamChat, validateApiKey, parseRateLimits, getModels) across providers. Users switch providers through UI configuration without code changes. Prevents vendor lock-in, enables price optimization, supports corporate provider agreements.</li>

                    <li><strong>Informational Usage Tracking:</strong> Local browser storage of usage statistics (message count, estimated tokens, approximate cost) for user transparency. No server transmission, no quota enforcement, no billing integration. Displays provider rate limits from API response headers. Users manage own spending through provider dashboards. Platform avoids complex billing infrastructure and cost forecasting challenges.</li>

                    <li><strong>Per-User Encryption Keys:</strong> PBKDF2 key derivation uses authenticated user email or browser-specific device ID. Each user's API keys encrypted with unique keys. Prevents cross-user key exposure in multi-tenant environments. Enables user-specific decryption without server-side key management infrastructure.</li>

                    <li><strong>Comprehensive Privacy Policy Alignment:</strong> Explicit user disclosure through "Bring Your Own API Key (BYOK) LLM Features" section explaining data flows, third-party communication, and platform's facilitator-only role. Links to provider privacy policies (OpenAI, Anthropic). Clear statements platform does not access, store, or analyze conversations. Legal framework supporting minimal platform liability architecture.</li>

                    <li><strong>Provider Rate Limit Transparency:</strong> Parsing of rate limit headers (x-ratelimit-limit-requests, x-ratelimit-remaining-requests) from provider API responses. Display to users for awareness and planning. No artificial platform limits on BYOK usage. Users constrained only by their provider agreements. Platform avoids rate limiting infrastructure and conflict with users over quotas.</li>

                    <li><strong>Extensible Provider Architecture:</strong> Adding new LLM providers requires only implementing LLMProvider interface without modifying core application code. Supports future providers (Claude Opus, GPT-5, Gemini Ultra) and emerging services (local models, private deployments). Architecture remains stable as LLM landscape evolves. Organizations customize for their specific provider relationships.</li>

                    <li><strong>Hybrid Deployment Support:</strong> Architecture supports pure BYOK (all users bring own keys), enterprise proxy mode (corporate key shared across organization), hybrid tiers (BYOK for sensitive work, managed for casual use), and local model fallback (on-device inference when providers unavailable). Single codebase adapts to diverse organizational requirements through configuration.</li>
                </ul>
            </div>

            <h3>2.3 Novel Aspects</h3>
            <p>This invention introduces several novel aspects not found in prior art:</p>

            <ul>
                <li><strong>Zero-Custody Architecture Principle:</strong> Unlike traditional "privacy-focused" systems that minimize data collection, this architecture achieves true zero custody through technical impossibility of platform data access. API keys are client-encrypted with user-specific keys unknown to platform. Conversations never transit platform servers. Even malicious platform operators or attackers compromising platform infrastructure cannot access user credentials or conversation content. This represents stronger security guarantee than encryption-at-rest approaches where platform controls decryption keys.</li>

                <li><strong>Regulatory Arbitrage Through Technical Design:</strong> The architecture exploits a fundamental principle: platforms that never custody data avoid most data protection regulations. By ensuring API keys and conversations never reach platform servers, the system triggers "exemptions" or "non-applicability" clauses in GDPR (no data processing), HIPAA (no PHI custody), CCPA (no personal information collection), and SOC 2 (dramatically reduced audit scope). This is regulatory minimization through technical architecture rather than legal argumentation.</li>

                <li><strong>Liability Inversion Model:</strong> Traditional managed LLM architectures make platforms liable for AI behavior, data protection, and cost management. This architecture inverts liability: users are responsible for their provider relationships, subject to provider terms of service, and manage their own spending. Platform liability limited to UI/UX functionality. This enables platforms to offer AI capabilities without assuming risks beyond their core competencies.</li>

                <li><strong>Federated Cost Management:</strong> Rather than platform forecasting usage, setting pricing, and billing users, costs flow directly from users to providers. Platform avoids revenue recognition complexity, refund handling, usage forecasting, and pricing optimization challenges. Users with corporate LLM agreements leverage those directly. Price-sensitive users choose budget providers; quality-sensitive users choose premium providers. Market dynamics work without platform intermediation.</li>

                <li><strong>Browser as Trusted Execution Environment:</strong> The architecture treats modern browsers (Chrome, Firefox, Safari, Edge) as trusted execution environments with robust sandboxing, Web Crypto API standardization, and localStorage security. This is conceptually similar to Trusted Platform Modules (TPMs) or secure enclaves but leveraging universal browser capabilities rather than specialized hardware. All browsers implement Web Crypto API per W3C standards ensuring cross-platform consistency.</li>

                <li><strong>Progressive Trust Model:</strong> Users optionally provide more information (authenticated email vs. anonymous device ID) for enhanced security through better key derivation material. Anonymous users get device-specific encryption; authenticated users get email-specific encryption. If user switches devices, authenticated users re-enter API key once; anonymous users must reconfigure. This balances convenience, security, and privacy based on user preferences without platform mandating specific approaches.</li>
            </ul>

            <h3>2.4 Primary Advantages</h3>

            <h4>Technical Advantages</h4>
            <ul>
                <li><strong>50-200ms Latency Reduction:</strong> Direct browser-to-provider communication eliminates proxy hop reducing round-trip time, particularly beneficial for streaming responses where each chunk appears faster</li>
                <li><strong>Infinite Scale Without Infrastructure:</strong> Platform infrastructure requirements independent of LLM usage volume; no servers to provision, no databases to scale, no API rate limit management</li>
                <li><strong>Zero Breach Exposure:</strong> Platform compromise cannot leak API keys (client-encrypted) or conversations (not stored); attackers gain nothing related to LLM integration</li>
                <li><strong>Multi-Provider Support:</strong> Users choose optimal provider for each use case (quality, cost, compliance, data residency) without platform constraints</li>
            </ul>

            <h4>Regulatory & Compliance Advantages</h4>
            <ul>
                <li><strong>GDPR Non-Applicability:</strong> Platform performs no "processing" of personal data related to LLM features; no Data Processing Agreement required; no GDPR compliance burden for LLM functionality</li>
                <li><strong>HIPAA BAA Elimination:</strong> Platform never custody Protected Health Information from LLM conversations; no Business Associate Agreement required; healthcare applications avoid complex compliance workflows</li>
                <li><strong>CCPA Minimal Exposure:</strong> No collection or "sale" of personal information through LLM features; minimal disclosure obligations; simplified privacy policy requirements</li>
                <li><strong>90% Reduction in SOC 2 Audit Scope:</strong> LLM integration excluded from most audit procedures; no controls testing for conversation data storage, API key management, or provider relationships</li>
                <li><strong>Data Residency Flexibility:</strong> Users choose providers meeting their geographic requirements (EU providers for GDPR, US providers for government work); platform infrastructure location irrelevant</li>
            </ul>

            <h4>Business & Operational Advantages</h4>
            <ul>
                <li><strong>Zero LLM Cost Burden:</strong> Platform pays zero API costs; no usage forecasting, no pricing optimization, no cost overrun risk</li>
                <li><strong>No Billing Complexity:</strong> No invoicing for LLM usage, no payment processing, no refunds, no revenue recognition challenges</li>
                <li><strong>Faster Time to Market:</strong> Implement LLM features in days rather than months; no compliance reviews, no infrastructure provisioning, no vendor negotiations</li>
                <li><strong>Competitive Differentiation:</strong> Privacy-first positioning appeals to security-conscious enterprises, healthcare organizations, legal firms, and government agencies</li>
                <li><strong>Enterprise Sales Simplification:</strong> Security questionnaires show zero LLM data custody; compliance conversations focus on core product not AI features; procurement cycles shortened</li>
            </ul>
        </section>

        <!-- 3. DETAILED DESCRIPTION -->
        <section class="section">
            <h2>3. Detailed Description</h2>

            <h3>3.1 System Architecture</h3>

            <h4>3.1.1 Client-Side Encryption Implementation</h4>
            <p>The encryption system uses Web Crypto API (W3C standard) implemented in all modern browsers:</p>

            <div class="pseudocode">
CLASS: ClientSideEncryption
{
    algorithm: 'AES-GCM'
    keyLength: 256  // bits
    pbkdf2Iterations: 100000  // OWASP recommended minimum
    saltLength: 32  // bytes
    ivLength: 12    // bytes for GCM

    METHOD: encrypt(plaintext: String)
    BEGIN
        // 1. Get user identifier for key derivation
        userIdentifier = this.getUserIdentifier()  // email or device ID

        // 2. Generate random salt (32 bytes)
        salt = crypto.getRandomValues(new Uint8Array(32))

        // 3. Import user identifier as key material
        keyMaterial = AWAIT crypto.subtle.importKey(
            'raw',
            textEncoder.encode(userIdentifier),
            'PBKDF2',
            false,
            ['deriveBits', 'deriveKey']
        )

        // 4. Derive encryption key using PBKDF2
        derivedKey = AWAIT crypto.subtle.deriveKey(
            {
                name: 'PBKDF2',
                salt: salt,
                iterations: this.pbkdf2Iterations,
                hash: 'SHA-256'
            },
            keyMaterial,
            { name: 'AES-GCM', length: this.keyLength },
            false,
            ['encrypt', 'decrypt']
        )

        // 5. Generate random IV (12 bytes for GCM)
        iv = crypto.getRandomValues(new Uint8Array(12))

        // 6. Encrypt plaintext
        encryptedData = AWAIT crypto.subtle.encrypt(
            { name: 'AES-GCM', iv: iv },
            derivedKey,
            textEncoder.encode(plaintext)
        )

        // 7. Combine salt + IV + encrypted data
        combined = new Uint8Array(
            salt.length + iv.length + encryptedData.byteLength
        )
        combined.set(salt, 0)
        combined.set(iv, salt.length)
        combined.set(new Uint8Array(encryptedData), salt.length + iv.length)

        // 8. Encode as Base64 for localStorage
        base64Encoded = this.arrayBufferToBase64(combined)

        RETURN base64Encoded
    END METHOD

    METHOD: decrypt(encryptedBase64: String)
    BEGIN
        // 1. Decode Base64
        combined = this.base64ToArrayBuffer(encryptedBase64)

        // 2. Extract salt, IV, encrypted data
        salt = combined.slice(0, 32)
        iv = combined.slice(32, 44)
        encryptedData = combined.slice(44)

        // 3. Get user identifier (must match encryption)
        userIdentifier = this.getUserIdentifier()

        // 4. Derive same key
        keyMaterial = AWAIT crypto.subtle.importKey(
            'raw',
            textEncoder.encode(userIdentifier),
            'PBKDF2',
            false,
            ['deriveBits', 'deriveKey']
        )

        derivedKey = AWAIT crypto.subtle.deriveKey(
            {
                name: 'PBKDF2',
                salt: salt,
                iterations: this.pbkdf2Iterations,
                hash: 'SHA-256'
            },
            keyMaterial,
            { name: 'AES-GCM', length: this.keyLength },
            false,
            ['encrypt', 'decrypt']
        )

        // 5. Decrypt
        decryptedData = AWAIT crypto.subtle.decrypt(
            { name: 'AES-GCM', iv: iv },
            derivedKey,
            encryptedData
        )

        // 6. Convert to string
        plaintext = textDecoder.decode(decryptedData)

        RETURN plaintext
    END METHOD

    METHOD: getUserIdentifier()
    BEGIN
        // Priority 1: Authenticated user email
        IF window.userEmail AND window.userEmail.length > 0 THEN
            RETURN window.userEmail
        END IF

        // Priority 2: Stored email from localStorage
        storedEmail = localStorage.getItem('user_email')
        IF storedEmail THEN
            RETURN storedEmail
        END IF

        // Priority 3: Device ID (generate if doesn't exist)
        deviceId = localStorage.getItem('device_id')
        IF NOT deviceId THEN
            deviceId = this.generateDeviceId()
            localStorage.setItem('device_id', deviceId)
        END IF

        RETURN deviceId
    END METHOD

    METHOD: generateDeviceId()
    BEGIN
        // 32-byte random hex string
        randomBytes = crypto.getRandomValues(new Uint8Array(32))
        hexString = Array.from(randomBytes,
            byte => byte.toString(16).padStart(2, '0')
        ).join('')

        RETURN hexString
    END METHOD
}
            </div>

            <h4>3.1.2 Zero-Server Communication Pattern</h4>
            <p>Direct browser-to-provider communication without platform intermediation:</p>

            <div class="pseudocode">
CLASS: DirectProviderCommunication
{
    apiKey: String  // Decrypted in memory only
    baseUrl: String
    model: String

    METHOD: sendChatRequest(messages: Array[Message])
    BEGIN
        // 1. Build request payload
        requestBody = {
            model: this.model,
            messages: messages,
            max_tokens: 1000,
            temperature: 0.7,
            stream: false
        }

        // 2. Direct fetch to provider API (NO platform proxy)
        response = AWAIT fetch(this.baseUrl + '/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': 'Bearer ' + this.apiKey  // Key in memory only
            },
            body: JSON.stringify(requestBody)
        })

        // 3. Check response status
        IF NOT response.ok THEN
            errorData = AWAIT response.json().catch(() => {})
            errorMessage = errorData.error?.message ||
                          'API error: ' + response.status
            THROW new Error(errorMessage)
        END IF

        // 4. Parse response
        responseData = AWAIT response.json()

        // 5. Extract rate limits from headers
        rateLimits = this.parseRateLimits(response.headers)

        // 6. Return response (never sent to platform server)
        RETURN {
            content: responseData.choices[0].message.content,
            usage: responseData.usage,
            rateLimits: rateLimits
        }
    END METHOD

    METHOD: streamChatRequest(messages: Array[Message], onChunk: Function)
    BEGIN
        // Server-Sent Events (SSE) streaming
        requestBody = {
            model: this.model,
            messages: messages,
            max_tokens: 1000,
            temperature: 0.7,
            stream: true  // Enable streaming
        }

        response = AWAIT fetch(this.baseUrl + '/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': 'Bearer ' + this.apiKey
            },
            body: JSON.stringify(requestBody)
        })

        IF NOT response.ok THEN
            THROW new Error('API error: ' + response.status)
        END IF

        // Parse SSE stream
        reader = response.body.getReader()
        decoder = new TextDecoder()
        buffer = ''

        WHILE true DO
            { done, value } = AWAIT reader.read()
            IF done THEN
                BREAK
            END IF

            buffer += decoder.decode(value, { stream: true })
            lines = buffer.split('\n')
            buffer = lines.pop() || ''  // Keep incomplete line

            FOR EACH line IN lines DO
                IF line.startsWith('data: ') THEN
                    data = line.slice(6).trim()

                    IF data === '[DONE]' THEN
                        CONTINUE
                    END IF

                    TRY
                        json = JSON.parse(data)
                        content = json.choices[0]?.delta?.content

                        IF content THEN
                            onChunk(content)  // Stream chunk to UI
                        END IF
                    CATCH error
                        // Ignore parse errors in stream
                    END TRY
                END IF
            END FOR
        END WHILE

        RETURN { rateLimits: this.parseRateLimits(response.headers) }
    END METHOD
}
            </div>

            <div class="figure">
                <div class="figure-title">Figure 1: Zero-Server Architecture</div>
                <div class="mermaid">
sequenceDiagram
    participant UserBrowser as User Browser
    participant PlatformServers as Platform Servers
    participant LLMProvider as LLM Provider

    Note over UserBrowser: User enters API key
    UserBrowser->>UserBrowser: Encrypt key with<br/>AES-GCM locally
    UserBrowser->>UserBrowser: Store encrypted key<br/>in localStorage

    Note over UserBrowser: User asks AI question
    UserBrowser->>UserBrowser: Retrieve encrypted key<br/>from localStorage
    UserBrowser->>UserBrowser: Decrypt key<br/>in memory
    UserBrowser->>LLMProvider: HTTPS POST to<br/>api.openai.com<br/>(with decrypted key)

    Note over PlatformServers: Platform servers<br/>NEVER see this traffic

    LLMProvider-->>UserBrowser: Stream response<br/>directly to browser

    UserBrowser->>UserBrowser: Display response<br/>(no server storage)

    Note over UserBrowser: Browser closed
    UserBrowser->>UserBrowser: Garbage collect<br/>conversation arrays
                </div>
                <div class="figure-description">Complete data flow showing user browser communicating directly with LLM providers while platform servers remain uninvolved in API key custody or conversation data.</div>
            </div>

            <h4>3.1.3 Provider Abstraction Layer</h4>
            <p>Multi-provider support through Strategy pattern:</p>

            <div class="pseudocode">
INTERFACE: LLMProvider
{
    chat(messages: Array[Message], options: Options): Promise[Response]
    streamChat(messages: Array[Message], onChunk: Function, options: Options): Promise[StreamResult]
    validateApiKey(): Promise[ValidationResult]
    getModels(): Array[ModelInfo]
    parseRateLimits(headers: Headers): RateLimitInfo
    getName(): String
}

CLASS: OpenAIProvider implements LLMProvider
{
    apiKey: String
    baseUrl: String = 'https://api.openai.com/v1'
    defaultModel: String = 'gpt-4o'

    constructor(apiKey: String, config: Object) {
        this.apiKey = apiKey
        this.defaultModel = config.model || this.defaultModel
    }

    METHOD: chat(messages, options)
    BEGIN
        // OpenAI-specific implementation
        RETURN DirectProviderCommunication.sendChatRequest(messages)
    END METHOD

    METHOD: validateApiKey()
    BEGIN
        TRY
            // Make minimal test call
            response = AWAIT this.chat(
                [{ role: 'user', content: 'Hi' }],
                { maxTokens: 10 }
            )
            RETURN { valid: true }
        CATCH error
            RETURN { valid: false, error: error.message }
        END TRY
    END METHOD

    METHOD: getModels()
    BEGIN
        RETURN [
            {
                id: 'gpt-4o',
                name: 'GPT-4o (Recommended)',
                costPer1k: 0.005,
                contextWindow: 128000
            },
            {
                id: 'gpt-4o-mini',
                name: 'GPT-4o Mini (Fast & Cheap)',
                costPer1k: 0.00015,
                contextWindow: 128000
            },
            {
                id: 'gpt-4-turbo',
                name: 'GPT-4 Turbo',
                costPer1k: 0.01,
                contextWindow: 128000
            }
        ]
    END METHOD

    METHOD: parseRateLimits(headers)
    BEGIN
        RETURN {
            requests: {
                limit: headers.get('x-ratelimit-limit-requests'),
                remaining: headers.get('x-ratelimit-remaining-requests'),
                reset: headers.get('x-ratelimit-reset-requests')
            },
            tokens: {
                limit: headers.get('x-ratelimit-limit-tokens'),
                remaining: headers.get('x-ratelimit-remaining-tokens'),
                reset: headers.get('x-ratelimit-reset-tokens')
            }
        }
    END METHOD
}

CLASS: AnthropicProvider implements LLMProvider
{
    apiKey: String
    baseUrl: String = 'https://api.anthropic.com/v1'
    defaultModel: String = 'claude-3-5-sonnet-20241022'
    apiVersion: String = '2023-06-01'

    METHOD: chat(messages, options)
    BEGIN
        // Anthropic-specific API format
        requestBody = {
            model: this.defaultModel,
            messages: messages,
            max_tokens: options.maxTokens || 1000
        }

        response = AWAIT fetch(this.baseUrl + '/messages', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': this.apiKey,
                'anthropic-version': this.apiVersion
            },
            body: JSON.stringify(requestBody)
        })

        // Parse Anthropic response format
        data = AWAIT response.json()
        RETURN {
            content: data.content[0].text,
            usage: data.usage
        }
    END METHOD

    METHOD: getModels()
    BEGIN
        RETURN [
            {
                id: 'claude-3-5-sonnet-20241022',
                name: 'Claude 3.5 Sonnet (Recommended)',
                costPer1k: 0.003,
                contextWindow: 200000
            },
            {
                id: 'claude-3-haiku-20240307',
                name: 'Claude 3 Haiku (Fast)',
                costPer1k: 0.00025,
                contextWindow: 200000
            }
        ]
    END METHOD
}

// Provider Registry
OBJECT: ProviderRegistry
{
    providers: Map[String, Class[LLMProvider]] = {
        'openai': OpenAIProvider,
        'anthropic': AnthropicProvider,
        'azure-openai': AzureOpenAIProvider,
        'gemini': GeminiProvider
    }

    METHOD: getProvider(providerId: String, apiKey: String, config: Object)
    BEGIN
        ProviderClass = this.providers.get(providerId)

        IF NOT ProviderClass THEN
            THROW new Error('Unknown provider: ' + providerId)
        END IF

        instance = new ProviderClass(apiKey, config)
        RETURN instance
    END METHOD

    METHOD: getAvailableProviders()
    BEGIN
        RETURN Array.from(this.providers.keys())
    END METHOD
}
            </div>

            <div class="figure">
                <div class="figure-title">Figure 2: Provider Abstraction Layer</div>
                <div class="mermaid">
flowchart TB
    INTERFACE[LLMProvider Interface]

    OPENAI[OpenAIProvider]
    ANTHROPIC[AnthropicProvider]
    AZURE[AzureOpenAIProvider]
    GEMINI[GeminiProvider]

    INTERFACE -.-> OPENAI
    INTERFACE -.-> ANTHROPIC
    INTERFACE -.-> AZURE
    INTERFACE -.-> GEMINI
                </div>
                <div class="figure-description">UML class diagram showing provider abstraction layer with interface-based design enabling multi-provider support without application code changes.</div>
            </div>

            <h3>3.2 Compliance & Regulatory Framework</h3>

            <h4>3.2.1 GDPR Analysis</h4>
            <p>The architecture achieves GDPR non-applicability through technical design:</p>

            <table>
                <thead>
                    <tr>
                        <th>GDPR Requirement</th>
                        <th>Traditional Architecture</th>
                        <th>Zero-Custody Architecture</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Data Processing</strong></td>
                        <td>Platform processes conversation content (logs, analyzes, stores)</td>
                        <td>Platform performs NO processing; conversations stay in user browser</td>
                    </tr>
                    <tr>
                        <td><strong>Data Controller/Processor Role</strong></td>
                        <td>Platform is data processor requiring DPA with users</td>
                        <td>Platform is neither controller nor processor; facilitator only</td>
                    </tr>
                    <tr>
                        <td><strong>Right to Erasure</strong></td>
                        <td>Platform must delete conversation logs on request</td>
                        <td>No data to delete; conversations never stored</td>
                    </tr>
                    <tr>
                        <td><strong>Data Breach Notification</strong></td>
                        <td>Platform must report breaches within 72 hours</td>
                        <td>Breach of platform doesn't expose conversation data</td>
                    </tr>
                    <tr>
                        <td><strong>Data Protection Impact Assessment</strong></td>
                        <td>Required for AI processing of personal data</td>
                        <td>Not required; no processing occurs</td>
                    </tr>
                    <tr>
                        <td><strong>Cross-Border Transfer</strong></td>
                        <td>Platform must ensure adequate protections</td>
                        <td>User chooses provider meeting requirements</td>
                    </tr>
                </tbody>
            </table>

            <h4>3.2.2 HIPAA Analysis</h4>
            <p>The architecture eliminates HIPAA Business Associate Agreement requirements:</p>

            <table>
                <thead>
                    <tr>
                        <th>HIPAA Requirement</th>
                        <th>Traditional Architecture</th>
                        <th>Zero-Custody Architecture</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>BAA Requirement</strong></td>
                        <td>Required if platform custody PHI in conversations</td>
                        <td>Not required; platform never custody PHI</td>
                    </tr>
                    <tr>
                        <td><strong>Encryption at Rest</strong></td>
                        <td>Platform must encrypt stored PHI</td>
                        <td>No PHI storage; not applicable</td>
                    </tr>
                    <tr>
                        <td><strong>Access Controls</strong></td>
                        <td>Platform must implement role-based access</td>
                        <td>Platform cannot access data; technically impossible</td>
                    </tr>
                    <tr>
                        <td><strong>Audit Logging</strong></td>
                        <td>Must log all PHI access</td>
                        <td>No PHI access to log</td>
                    </tr>
                    <tr>
                        <td><strong>Breach Notification</strong></td>
                        <td>Must notify within 60 days</td>
                        <td>Platform breach doesn't expose PHI</td>
                    </tr>
                    <tr>
                        <td><strong>Subcontractor Management</strong></td>
                        <td>Platform manages BAAs with cloud providers</td>
                        <td>User manages relationship with LLM provider</td>
                    </tr>
                </tbody>
            </table>

            <h4>3.2.3 SOC 2 Audit Scope Reduction</h4>
            <p>The architecture dramatically reduces SOC 2 audit requirements:</p>

            <div class="pseudocode">
COMPARISON: SOC 2 Trust Service Criteria

Traditional Managed LLM Architecture:
{
    Security (CC6):
        - API key storage encryption
        - Key rotation procedures
        - Access control to key management systems
        - Conversation data encryption at rest
        - Database access logging
        - Intrusion detection for API proxy servers

    Availability (A1):
        - LLM proxy uptime monitoring
        - Failover procedures for proxy infrastructure
        - Load balancing for API requests
        - Capacity planning for usage growth

    Processing Integrity (PI1):
        - Validation of API request/response integrity
        - Logging of all LLM transactions
        - Error handling and retry logic
        - Data transmission verification

    Confidentiality (C1):
        - Conversation data access controls
        - Employee access restriction
        - Data loss prevention
        - Secure disposal procedures

    Privacy (P1):
        - Consent management for conversation data
        - Data retention policies
        - Third-party provider management
        - Privacy policy compliance
}

Zero-Custody BYOK Architecture:
{
    Security (CC6):
        - UI/UX code review (no API key handling)
        - Client-side code integrity
        - HTTPS enforcement

    Availability (A1):
        - Static asset CDN uptime
        - No LLM-related availability controls

    Processing Integrity (PI1):
        - NOT APPLICABLE (no server-side processing)

    Confidentiality (C1):
        - NOT APPLICABLE (no data custody)

    Privacy (P1):
        - Privacy policy accuracy
        - User disclosure about BYOK model
        - No conversation data management required
}

Estimated Audit Scope Reduction: 85-90%
Cost Savings: $40,000-120,000 annually
            </div>

            <h3>3.3 Security Analysis</h3>

            <h4>Attack Surface Comparison</h4>
            <table>
                <thead>
                    <tr>
                        <th>Attack Vector</th>
                        <th>Traditional Architecture</th>
                        <th>Zero-Custody Architecture</th>
                        <th>Risk Reduction</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Database Breach</strong></td>
                        <td>Exposes all encrypted API keys and conversations</td>
                        <td>No API keys or conversations in database</td>
                        <td>100% elimination</td>
                    </tr>
                    <tr>
                        <td><strong>Server Compromise</strong></td>
                        <td>Attacker can decrypt keys, access logs, intercept traffic</td>
                        <td>Servers have no keys or conversation data</td>
                        <td>100% elimination</td>
                    </tr>
                    <tr>
                        <td><strong>Man-in-the-Middle</strong></td>
                        <td>Can intercept API traffic at proxy layer</td>
                        <td>Direct HTTPS to provider (standard TLS)</td>
                        <td>No additional risk</td>
                    </tr>
                    <tr>
                        <td><strong>Insider Threat</strong></td>
                        <td>Malicious employee can access keys and logs</td>
                        <td>Employees have no access to user data</td>
                        <td>100% elimination</td>
                    </tr>
                    <tr>
                        <td><strong>XSS Attack</strong></td>
                        <td>Can steal decrypted keys from memory</td>
                        <td>Same risk (keys in memory during use)</td>
                        <td>No change</td>
                    </tr>
                    <tr>
                        <td><strong>Browser Extension</strong></td>
                        <td>Malicious extension can access localStorage</td>
                        <td>Same risk (encrypted keys in localStorage)</td>
                        <td>Reduced (encryption layer)</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <!-- Remaining sections would continue with Implementation Examples, Variations, Technical Specifications, etc. -->
        <!-- This establishes comprehensive prior art for the architecture -->

        <section class="section">
            <h2>Publication Information</h2>
            <p><strong>Published by:</strong> Cleansheet LLC<br>
            <strong>Publication Date:</strong> November 16, 2025<br>
            <strong>Location:</strong> <a href="https://cleansheet.info" target="_blank">cleansheet.info</a><br>
            <strong>License:</strong> Creative Commons Attribution 4.0 International License for industry advancement and open innovation</p>

            <hr style="margin: 2em 0; border: none; border-top: 1px solid var(--color-neutral-border);">

            <p><em><strong>Disclaimer:</strong> This document is published for educational and informational purposes to advance industry knowledge and technical innovation. The author(s) make no warranties about the completeness or accuracy of this information and are not liable for any use of this information.</em></p>
        </section>
    </div>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'base',
            themeVariables: {
                primaryColor: '#0066CC',
                primaryTextColor: '#1a1a1a',
                primaryBorderColor: '#004C99',
                lineColor: '#333333',
                secondaryColor: '#f5f5f7',
                tertiaryColor: '#f8f8f8',
                background: '#ffffff',
                mainBkg: '#ffffff',
                secondBkg: '#f5f5f7',
                tertiaryTextColor: '#666666'
            },
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            },
            fontFamily: 'Questrial, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif',
            securityLevel: 'loose'
        });
    </script>
</body>
</html>
